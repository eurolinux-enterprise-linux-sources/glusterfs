From f1c007eae8ab4a007b04b1727a1d5f0a1cecfa1f Mon Sep 17 00:00:00 2001
From: Mohammed Rafi KC <rkavunga@redhat.com>
Date: Tue, 8 Sep 2015 13:04:45 +0530
Subject: [PATCH 16/23] tier/shd: make shd commands compatible with tiering

tiering volfiles may contain afr and disperse together
or multiple time based on configuration. And the
informations for those configurations are stored in
tier_info. So most of the volgen code generation
need to be changed to make compatible with it.

Back port of>
>Change-Id: I563d1ca6f281f59090ebd470b7fda1cc4b1b7e1d
>BUG: 1261276
>Signed-off-by: Mohammed Rafi KC <rkavunga@redhat.com>
>Reviewed-on: http://review.gluster.org/12135
>Tested-by: NetBSD Build System <jenkins@build.gluster.org>
>Tested-by: Gluster Build System <jenkins@build.gluster.com>
>Reviewed-by: Pranith Kumar Karampuri <pkarampu@redhat.com>

>(cherry picked from commit 0ef62933649392051e73fe01c028e41baddec489)

>BUG: 1261744
>Change-Id: Iff1b27ae8ce61f1f38fbbd6c92894b3d3516e4d4
>Signed-off-by: Mohammed Rafi KC <rkavunga@redhat.com>
>Reviewed-on: http://review.gluster.org/12344
>Tested-by: NetBSD Build System <jenkins@build.gluster.org>
>Tested-by: Gluster Build System <jenkins@build.gluster.com>
>Reviewed-by: Joseph Fernandes
>Reviewed-by: Dan Lambright <dlambrig@redhat.com>
>Tested-by: Dan Lambright <dlambrig@redhat.com>

Change-Id: Ic820d7d923109d91f226cb6ee93d3f888c3c990a
BUG: 1271733
Signed-off-by: Mohammed Rafi KC <rkavunga@redhat.com>
Reviewed-on: https://code.engineering.redhat.com/gerrit/59405
Reviewed-by: Dan Lambright <dlambrig@redhat.com>
Tested-by: Dan Lambright <dlambrig@redhat.com>
---
 tests/basic/tier/tier-heald.t                   |   97 +++++++++
 xlators/mgmt/glusterd/src/glusterd-op-sm.c      |  243 ++++++++++++++---------
 xlators/mgmt/glusterd/src/glusterd-utils.c      |   18 ++-
 xlators/mgmt/glusterd/src/glusterd-volgen.c     |   91 ++++++++--
 xlators/mgmt/glusterd/src/glusterd-volgen.h     |    5 +-
 xlators/mgmt/glusterd/src/glusterd-volume-ops.c |   90 ++++++++--
 xlators/mgmt/glusterd/src/glusterd-volume-set.c |   10 +
 7 files changed, 428 insertions(+), 126 deletions(-)
 create mode 100644 tests/basic/tier/tier-heald.t

diff --git a/tests/basic/tier/tier-heald.t b/tests/basic/tier/tier-heald.t
new file mode 100644
index 0000000..05b19e4
--- /dev/null
+++ b/tests/basic/tier/tier-heald.t
@@ -0,0 +1,97 @@
+#!/bin/bash
+
+. $(dirname $0)/../../include.rc
+. $(dirname $0)/../../volume.rc
+
+# This test contains volume heal commands handled by glusterd.
+# Covers enable/disable at the moment. Will be enhanced later to include
+# the other commands as well.
+
+cleanup;
+TEST glusterd
+TEST pidof glusterd
+
+volfile=$(gluster system:: getwd)"/glustershd/glustershd-server.vol"
+
+# Commands should fail when both tiers are not of distribute type.
+# Glustershd shouldn't be running as long as there are no replicate/disperse
+# volumes
+TEST $CLI volume create dist_tier $H0:$B0/cold
+TEST $CLI volume start dist_tier
+TEST $CLI volume attach-tier dist_tier $H0:$B0/hot
+
+TEST "[ -z $(get_shd_process_pid)]"
+TEST ! $CLI volume heal dist_tier enable
+TEST ! $CLI volume heal dist_tier disable
+
+# Commands should work on replicate/disperse volume.
+TEST $CLI volume create r2 replica 2 $H0:$B0/r2_0 $H0:$B0/r2_1
+TEST "[ -z $(get_shd_process_pid)]"
+TEST $CLI volume start r2
+
+TEST $CLI volume attach-tier r2 $H0:$B0/r2_hot
+
+EXPECT_WITHIN $PROCESS_UP_TIMEOUT "[0-9][0-9]*" get_shd_process_pid
+TEST $CLI volume heal r2 enable
+EXPECT "enable" volume_option r2 "cluster.self-heal-daemon"
+EXPECT "enable" volgen_volume_option $volfile r2-replicate-0 cluster replicate self-heal-daemon
+EXPECT_WITHIN $PROCESS_UP_TIMEOUT "[0-9][0-9]*" get_shd_process_pid
+TEST $CLI volume heal r2 disable
+EXPECT "disable" volume_option r2 "cluster.self-heal-daemon"
+EXPECT "disable" volgen_volume_option $volfile r2-replicate-0 cluster replicate self-heal-daemon
+EXPECT_WITHIN $PROCESS_UP_TIMEOUT "[0-9][0-9]*" get_shd_process_pid
+# Commands should work on disperse volume.
+TEST $CLI volume create ec2 disperse 3 redundancy 1 $H0:$B0/ec2_0 $H0:$B0/ec2_1 $H0:$B0/ec2_2
+TEST $CLI volume start ec2
+
+TEST $CLI volume attach-tier ec2 replica 2 $H0:$B0/ec2_hot{1..4}
+EXPECT_WITHIN $PROCESS_UP_TIMEOUT "[0-9][0-9]*" get_shd_process_pid
+TEST $CLI volume heal ec2 enable
+EXPECT "enable" volume_option ec2 "cluster.disperse-self-heal-daemon"
+EXPECT "enable" volgen_volume_option $volfile ec2-disperse-0 cluster disperse self-heal-daemon
+EXPECT_WITHIN $PROCESS_UP_TIMEOUT "[0-9][0-9]*" get_shd_process_pid
+TEST $CLI volume heal ec2 disable
+EXPECT "disable" volume_option ec2 "cluster.disperse-self-heal-daemon"
+EXPECT "disable" volgen_volume_option $volfile ec2-disperse-0 cluster disperse self-heal-daemon
+EXPECT_WITHIN $PROCESS_UP_TIMEOUT "[0-9][0-9]*" get_shd_process_pid
+
+#Check that shd graph is rewritten correctly on volume stop/start
+EXPECT "Y" volgen_volume_exists $volfile ec2-disperse-0 cluster disperse
+EXPECT "Y" volgen_volume_exists $volfile r2-replicate-0 cluster replicate
+TEST $CLI volume stop r2
+EXPECT "Y" volgen_volume_exists $volfile ec2-disperse-0 cluster disperse
+EXPECT "N" volgen_volume_exists $volfile r2-replicate-0 cluster replicate
+TEST $CLI volume stop ec2
+# When both the volumes are stopped glustershd volfile is not modified just the
+# process is stopped
+TEST "[ -z $(get_shd_process_pid) ]"
+
+TEST $CLI volume start r2
+EXPECT "N" volgen_volume_exists $volfile ec2-disperse-0 cluster disperse
+EXPECT "Y" volgen_volume_exists $volfile r2-replicate-0 cluster replicate
+
+TEST $CLI volume start ec2
+
+EXPECT "Y" volgen_volume_exists $volfile ec2-disperse-0 cluster disperse
+EXPECT "Y" volgen_volume_exists $volfile ec2-replicate-0 cluster replicate
+
+TEST $CLI volume detach-tier ec2 force
+
+EXPECT "Y" volgen_volume_exists $volfile ec2-disperse-0 cluster disperse
+EXPECT "N" volgen_volume_exists $volfile ec2-replicate-0 cluster replicate
+
+TEST $CLI volume set r2 self-heal-daemon on
+TEST $CLI volume set r2 cluster.self-heal-daemon off
+TEST ! $CLI volume set ec2 self-heal-daemon off
+TEST ! $CLI volume set ec2 cluster.self-heal-daemon on
+TEST ! $CLI volume set dist self-heal-daemon off
+TEST ! $CLI volume set dist cluster.self-heal-daemon on
+
+TEST $CLI volume set ec2 disperse-self-heal-daemon off
+TEST $CLI volume set ec2 cluster.disperse-self-heal-daemon on
+TEST ! $CLI volume set r2 disperse-self-heal-daemon on
+TEST ! $CLI volume set r2 cluster.disperse-self-heal-daemon off
+TEST ! $CLI volume set dist disperse-self-heal-daemon off
+TEST ! $CLI volume set dist cluster.disperse-self-heal-daemon on
+
+cleanup
diff --git a/xlators/mgmt/glusterd/src/glusterd-op-sm.c b/xlators/mgmt/glusterd/src/glusterd-op-sm.c
index 3d193c7..679a6c9 100644
--- a/xlators/mgmt/glusterd/src/glusterd-op-sm.c
+++ b/xlators/mgmt/glusterd/src/glusterd-op-sm.c
@@ -1542,7 +1542,6 @@ glusterd_op_stage_status_volume (dict_t *dict, char **op_errstr)
         char                   msg[2048]      = {0,};
         char                  *volname        = NULL;
         char                  *brick          = NULL;
-        char                  *shd_key        = NULL;
         xlator_t              *this           = NULL;
         glusterd_conf_t       *priv           = NULL;
         glusterd_brickinfo_t  *brickinfo      = NULL;
@@ -1623,10 +1622,7 @@ glusterd_op_stage_status_volume (dict_t *dict, char **op_errstr)
                 }
         } else if ((cmd & GF_CLI_STATUS_SHD) != 0) {
                 if (glusterd_is_shd_compatible_volume (volinfo)) {
-                        shd_key = volgen_get_shd_key (volinfo);
-                        shd_enabled = dict_get_str_boolean (vol_opts,
-                                                     shd_key,
-                                                     _gf_true);
+                        shd_enabled = is_self_heal_enabled (volinfo, vol_opts);
                 } else {
                         ret = -1;
                         snprintf (msg, sizeof (msg),
@@ -3081,7 +3077,6 @@ glusterd_op_status_volume (dict_t *dict, char **op_errstr,
         uint32_t                cmd             = 0;
         char                   *volname         = NULL;
         char                   *brick           = NULL;
-        char                   *shd_key         = NULL;
         xlator_t               *this            = NULL;
         glusterd_volinfo_t     *volinfo         = NULL;
         glusterd_brickinfo_t   *brickinfo       = NULL;
@@ -3254,12 +3249,9 @@ glusterd_op_status_volume (dict_t *dict, char **op_errstr,
                                 other_count++;
                                 node_count++;
                         }
-                        if (glusterd_is_shd_compatible_volume (volinfo)) {
-                                shd_key = volgen_get_shd_key (volinfo);
-                                shd_enabled = dict_get_str_boolean (vol_opts,
-                                                     shd_key,
-                                                     _gf_true);
-                        }
+
+                        if (glusterd_is_shd_compatible_volume (volinfo))
+                                shd_enabled = is_self_heal_enabled (volinfo, vol_opts);
                         if (shd_enabled) {
                                 ret = glusterd_add_node_to_dict
                                         (priv->shd_svc.name, rsp_dict,
@@ -6057,13 +6049,13 @@ out:
 
 int
 _select_hxlator_with_matching_brick (xlator_t *this,
-                                     glusterd_volinfo_t *volinfo, dict_t *dict)
+                                     glusterd_volinfo_t *volinfo, dict_t *dict,
+                                     int *index)
 {
         char                    *hostname = NULL;
         char                    *path = NULL;
         glusterd_brickinfo_t    *brickinfo = NULL;
         glusterd_conf_t         *priv   = NULL;
-        int                     index = 1;
         int                     hxl_children = 0;
 
         priv = this->private;
@@ -6073,6 +6065,8 @@ _select_hxlator_with_matching_brick (xlator_t *this,
                 return -1;
 
         hxl_children = _get_hxl_children_count (volinfo);
+        if ((*index) == 0)
+                (*index)++;
 
         cds_list_for_each_entry (brickinfo, &volinfo->bricks, brick_list) {
                 if (gf_uuid_is_null (brickinfo->uuid))
@@ -6080,22 +6074,21 @@ _select_hxlator_with_matching_brick (xlator_t *this,
 
                 if (!gf_uuid_compare (MY_UUID, brickinfo->uuid)) {
                         _add_hxlator_to_dict (dict, volinfo,
-                                              (index - 1)/hxl_children, 0);
+                                              ((*index) - 1)/hxl_children, 0);
                         return 1;
                 }
-                index++;
+                (*index)++;
         }
 
         return 0;
 }
-int
+void
 _select_hxlators_with_local_bricks (xlator_t *this, glusterd_volinfo_t *volinfo,
-                                    dict_t *dict)
+                                    dict_t *dict, int *index,
+                                    int *hxlator_count)
 {
         glusterd_brickinfo_t    *brickinfo = NULL;
         glusterd_conf_t         *priv   = NULL;
-        int                     index = 1;
-        int                     hxlator_count = 0;
         int                     hxl_children = 0;
         gf_boolean_t            add     = _gf_false;
         int                     cmd_replica_index = -1;
@@ -6103,6 +6096,9 @@ _select_hxlators_with_local_bricks (xlator_t *this, glusterd_volinfo_t *volinfo,
         priv = this->private;
         hxl_children = _get_hxl_children_count (volinfo);
 
+        if ((*index) == 0)
+                (*index)++;
+
         cds_list_for_each_entry (brickinfo, &volinfo->bricks, brick_list) {
                 if (gf_uuid_is_null (brickinfo->uuid))
                         (void)glusterd_resolve_brick (brickinfo);
@@ -6110,35 +6106,35 @@ _select_hxlators_with_local_bricks (xlator_t *this, glusterd_volinfo_t *volinfo,
                 if (!gf_uuid_compare (MY_UUID, brickinfo->uuid))
                         add = _gf_true;
 
-                if (index % hxl_children == 0) {
+                if ((*index) % hxl_children == 0) {
                         if (add) {
-                                _add_hxlator_to_dict (dict, volinfo,
-                                                      (index - 1)/hxl_children,
-                                                      hxlator_count);
-                                hxlator_count++;
+                              _add_hxlator_to_dict (dict, volinfo,
+                                                    ((*index) - 1)/hxl_children,
+                                                    (*hxlator_count));
+                                (*hxlator_count)++;
                         }
                         add = _gf_false;
                 }
 
-                index++;
+                (*index)++;
         }
 
-        return hxlator_count;
 }
 
 int
 _select_hxlators_for_full_self_heal (xlator_t *this,
                                      glusterd_volinfo_t *volinfo,
-                                     dict_t *dict)
+                                     dict_t *dict, int *index,
+                                     int *hxlator_count)
 {
         glusterd_brickinfo_t    *brickinfo = NULL;
         glusterd_conf_t         *priv   = NULL;
-        int                     index = 1;
-        int                     hxlator_count = 0;
         int                     hxl_children = 0;
         uuid_t                  candidate = {0};
 
         priv = this->private;
+        if ((*index) == 0)
+                (*index)++;
         if (volinfo->type == GF_CLUSTER_TYPE_DISPERSE) {
                 hxl_children = volinfo->disperse_count;
         } else {
@@ -6152,19 +6148,19 @@ _select_hxlators_for_full_self_heal (xlator_t *this,
                 if (gf_uuid_compare (brickinfo->uuid, candidate) > 0)
                         gf_uuid_copy (candidate, brickinfo->uuid);
 
-                if (index % hxl_children == 0) {
+                if ((*index) % hxl_children == 0) {
                         if (!gf_uuid_compare (MY_UUID, candidate)) {
                                 _add_hxlator_to_dict (dict, volinfo,
-                                                      (index-1)/hxl_children,
-                                                      hxlator_count);
-                                hxlator_count++;
+                                                      ((*index)-1)/hxl_children,
+                                                      (*hxlator_count));
+                                (*hxlator_count)++;
                         }
                         gf_uuid_clear (candidate);
                 }
 
-                index++;
+                (*index)++;
         }
-        return hxlator_count;
+        return *hxlator_count;
 }
 
 
@@ -6225,13 +6221,13 @@ out:
 
 static int
 fill_shd_status_for_local_bricks (dict_t *dict, glusterd_volinfo_t *volinfo,
-                                  cli_cmd_type type, dict_t *req_dict)
+                                  cli_cmd_type type, int *index,
+                                  dict_t *req_dict)
 {
         glusterd_brickinfo_t    *brickinfo = NULL;
         char                    msg[1024] = {0,};
         char                    key[1024]  = {0,};
         char                    value[1024] = {0,};
-        int                     index = 0;
         int                     ret = 0;
         xlator_t               *this = NULL;
         int                     cmd_replica_index = -1;
@@ -6257,18 +6253,18 @@ fill_shd_status_for_local_bricks (dict_t *dict, glusterd_volinfo_t *volinfo,
                         (void)glusterd_resolve_brick (brickinfo);
 
                 if (gf_uuid_compare (MY_UUID, brickinfo->uuid)) {
-                        index++;
+                        (*index)++;
                         continue;
                 }
 
                 if (type == PER_HEAL_XL) {
-                      if (cmd_replica_index != (index/volinfo->replica_count)) {
-                              index++;
+                   if (cmd_replica_index != ((*index)/volinfo->replica_count)) {
+                              (*index)++;
                               continue;
-                        }
+                   }
 
                 }
-                snprintf (key, sizeof (key), "%d-status",index);
+                snprintf (key, sizeof (key), "%d-status", (*index));
                 snprintf (value, sizeof (value), "%s %s",msg,
                           uuid_utoa(MY_UUID));
                 ret = dict_set_dynstr (dict, key, gf_strdup(value));
@@ -6278,7 +6274,7 @@ fill_shd_status_for_local_bricks (dict_t *dict, glusterd_volinfo_t *volinfo,
                                 "set the dictionary for shd status msg");
                         goto out;
                 }
-                snprintf (key, sizeof (key), "%d-shd-status",index);
+                snprintf (key, sizeof (key), "%d-shd-status", (*index));
                 ret = dict_set_str (dict, key, "off");
                 if (ret) {
                         gf_msg (this->name, GF_LOG_ERROR, 0,
@@ -6287,58 +6283,29 @@ fill_shd_status_for_local_bricks (dict_t *dict, glusterd_volinfo_t *volinfo,
                         goto out;
                 }
 
-                index++;
+                (*index)++;
         }
 
 out:
         return ret;
 
 }
-
-static int
-glusterd_bricks_select_heal_volume (dict_t *dict, char **op_errstr,
-                                    struct cds_list_head *selected,
-                                    dict_t *rsp_dict)
+int
+glusterd_shd_select_brick_xlator (dict_t *dict, gf_xl_afr_op_t heal_op,
+                                  glusterd_volinfo_t *volinfo, int *index,
+                                  int *hxlator_count, dict_t *rsp_dict)
 {
         int                                     ret = -1;
-        char                                    *volname = NULL;
         glusterd_conf_t                         *priv = NULL;
-        glusterd_volinfo_t                      *volinfo = NULL;
         xlator_t                                *this = NULL;
         char                                    msg[2048] = {0,};
         glusterd_pending_node_t                 *pending_node = NULL;
-        gf_xl_afr_op_t                          heal_op = GF_SHD_OP_INVALID;
-        int                                     hxlator_count = 0;
 
         this = THIS;
         GF_ASSERT (this);
         priv = this->private;
         GF_ASSERT (priv);
 
-        ret = dict_get_str (dict, "volname", &volname);
-        if (ret) {
-                gf_msg ("glusterd", GF_LOG_ERROR, 0,
-                        GD_MSG_DICT_GET_FAILED, "volume name get failed");
-                goto out;
-        }
-
-        ret = glusterd_volinfo_find (volname, &volinfo);
-        if (ret) {
-                snprintf (msg, sizeof (msg), "Volume %s does not exist",
-                          volname);
-
-                *op_errstr = gf_strdup (msg);
-                gf_msg ("glusterd", GF_LOG_ERROR, 0,
-                        GD_MSG_VOL_NOT_FOUND, "%s", msg);
-                goto out;
-        }
-
-        ret = dict_get_int32 (dict, "heal-op", (int32_t*)&heal_op);
-        if (ret || (heal_op == GF_SHD_OP_INVALID)) {
-                gf_msg ("glusterd", GF_LOG_ERROR, 0,
-                        GD_MSG_DICT_GET_FAILED, "heal op invalid");
-                goto out;
-        }
 
         switch (heal_op) {
         case GF_SHD_OP_INDEX_SUMMARY:
@@ -6354,6 +6321,7 @@ glusterd_bricks_select_heal_volume (dict_t *dict, char **op_errstr,
                 ret = fill_shd_status_for_local_bricks (rsp_dict,
                                                         volinfo,
                                                         ALL_HEAL_XL,
+                                                        index,
                                                         dict);
                 if (ret)
                         gf_msg (this->name, GF_LOG_ERROR, 0,
@@ -6375,6 +6343,7 @@ glusterd_bricks_select_heal_volume (dict_t *dict, char **op_errstr,
                 ret = fill_shd_status_for_local_bricks (rsp_dict,
                                                         volinfo,
                                                         PER_HEAL_XL,
+                                                        index,
                                                         dict);
                 if (ret)
                         gf_msg (this->name, GF_LOG_ERROR, 0,
@@ -6393,35 +6362,104 @@ glusterd_bricks_select_heal_volume (dict_t *dict, char **op_errstr,
 
         switch (heal_op) {
         case GF_SHD_OP_HEAL_FULL:
-                hxlator_count = _select_hxlators_for_full_self_heal (this,
-                                                                     volinfo,
-                                                                     dict);
+                _select_hxlators_for_full_self_heal (this, volinfo, dict,
+                                                     index, hxlator_count);
                 break;
         case GF_SHD_OP_STATISTICS_HEAL_COUNT_PER_REPLICA:
-                hxlator_count = _select_hxlator_with_matching_brick (this,
-                                                                     volinfo,
-                                                                     dict);
+                 (*hxlator_count) += _select_hxlator_with_matching_brick (this,
+                                                                       volinfo,
+                                                                       dict,
+                                                                       index);
                 break;
         default:
-                hxlator_count = _select_hxlators_with_local_bricks (this,
-                                                                    volinfo,
-                                                                    dict);
+                 _select_hxlators_with_local_bricks (this, volinfo, dict,
+                                                      index, hxlator_count);
                 break;
         }
+        ret = (*hxlator_count);
+out:
+        return ret;
+}
 
-        if (!hxlator_count)
+
+static int
+glusterd_bricks_select_heal_volume (dict_t *dict, char **op_errstr,
+                                    struct cds_list_head *selected,
+                                    dict_t *rsp_dict)
+{
+        int                                     ret = -1;
+        char                                    *volname = NULL;
+        glusterd_conf_t                         *priv = NULL;
+        glusterd_volinfo_t                      *volinfo = NULL;
+        glusterd_volinfo_t                      *dup_volinfo = NULL;
+        xlator_t                                *this = NULL;
+        char                                    msg[2048] = {0,};
+        glusterd_pending_node_t                 *pending_node = NULL;
+        gf_xl_afr_op_t                          heal_op = GF_SHD_OP_INVALID;
+        int                                     hxlator_count = 0;
+        int                                     index = 0;
+
+        this = THIS;
+        GF_ASSERT (this);
+        priv = this->private;
+        GF_ASSERT (priv);
+
+        ret = dict_get_str (dict, "volname", &volname);
+        if (ret) {
+                gf_msg ("glusterd", GF_LOG_ERROR, 0,
+                        GD_MSG_DICT_GET_FAILED, "volume name get failed");
                 goto out;
-        if (hxlator_count == -1) {
-                gf_msg (this->name, GF_LOG_ERROR, 0,
-                        GD_MSG_XLATOR_COUNT_GET_FAIL, "Could not determine the"
-                        "translator count");
-                ret = -1;
+        }
+
+        ret = glusterd_volinfo_find (volname, &volinfo);
+        if (ret) {
+                snprintf (msg, sizeof (msg), "Volume %s does not exist",
+                          volname);
+
+                *op_errstr = gf_strdup (msg);
+                gf_msg ("glusterd", GF_LOG_ERROR, 0,
+                        GD_MSG_VOL_NOT_FOUND, "%s", msg);
                 goto out;
         }
 
-        ret = dict_set_int32 (dict, "count", hxlator_count);
-        if (ret)
+        ret = dict_get_int32 (dict, "heal-op", (int32_t *)&heal_op);
+        if (ret || (heal_op == GF_SHD_OP_INVALID)) {
+                gf_msg ("glusterd", GF_LOG_ERROR, 0,
+                        GD_MSG_DICT_GET_FAILED, "heal op invalid");
                 goto out;
+        }
+        if (volinfo->type == GF_CLUSTER_TYPE_TIER) {
+                ret = glusterd_create_sub_tier_volinfo (volinfo, &dup_volinfo,
+                                                         _gf_false, volname);
+                if (ret < 0)
+                        goto out;
+
+                ret = glusterd_shd_select_brick_xlator (dict, heal_op,
+                                                        dup_volinfo,
+                                                        &index, &hxlator_count,
+                                                        rsp_dict);
+                glusterd_volinfo_delete (dup_volinfo);
+                if (ret < 0)
+                        goto out;
+                ret = glusterd_create_sub_tier_volinfo (volinfo, &dup_volinfo,
+                                                        _gf_true, volname);
+                if (ret < 0)
+                        goto out;
+                ret = glusterd_shd_select_brick_xlator (dict, heal_op,
+                                                        dup_volinfo,
+                                                        &index, &hxlator_count,
+                                                        rsp_dict);
+                glusterd_volinfo_delete (dup_volinfo);
+                if (ret < 0)
+                        goto out;
+        } else {
+                ret = glusterd_shd_select_brick_xlator (dict, heal_op,
+                                                        volinfo,
+                                                        &index, &hxlator_count,
+                                                        rsp_dict);
+                if (ret < 0)
+                        goto out;
+        }
 
         pending_node = GF_CALLOC (1, sizeof (*pending_node),
                                   gf_gld_mt_pending_node_t);
@@ -6435,6 +6473,21 @@ glusterd_bricks_select_heal_volume (dict_t *dict, char **op_errstr,
                 pending_node = NULL;
         }
 
+        if (!hxlator_count)
+                goto out;
+        if (hxlator_count == -1) {
+                gf_msg (this->name, GF_LOG_ERROR, 0,
+                        GD_MSG_XLATOR_COUNT_GET_FAIL, "Could not determine the"
+                        "translator count");
+                ret = -1;
+                goto out;
+        }
+
+        ret = dict_set_int32 (dict, "count", hxlator_count);
+        if (ret)
+                goto out;
+
+
 out:
         gf_msg_debug (THIS->name, 0, "Returning ret %d", ret);
         return ret;
diff --git a/xlators/mgmt/glusterd/src/glusterd-utils.c b/xlators/mgmt/glusterd/src/glusterd-utils.c
index 1994e74..b66cccd 100644
--- a/xlators/mgmt/glusterd/src/glusterd-utils.c
+++ b/xlators/mgmt/glusterd/src/glusterd-utils.c
@@ -6372,14 +6372,24 @@ out:
         return ret;
 }
 
+static inline int
+is_replica_volume (int type)
+{
+        if (type == GF_CLUSTER_TYPE_REPLICATE ||
+            type == GF_CLUSTER_TYPE_STRIPE_REPLICATE)
+                return 1;
+        return 0;
+}
 gf_boolean_t
 glusterd_is_volume_replicate (glusterd_volinfo_t *volinfo)
 {
         gf_boolean_t    replicates = _gf_false;
-        if (volinfo && ((volinfo->type == GF_CLUSTER_TYPE_REPLICATE) ||
-            (volinfo->type == GF_CLUSTER_TYPE_STRIPE_REPLICATE)))
-                replicates = _gf_true;
-        return replicates;
+        if (volinfo->type == GF_CLUSTER_TYPE_TIER) {
+                replicates = is_replica_volume (volinfo->tier_info.cold_type) |
+                             is_replica_volume (volinfo->tier_info.hot_type);
+                return replicates;
+       }
+       return is_replica_volume ((volinfo->type));
 }
 
 gf_boolean_t
diff --git a/xlators/mgmt/glusterd/src/glusterd-volgen.c b/xlators/mgmt/glusterd/src/glusterd-volgen.c
index d531ad3..bf55d52 100644
--- a/xlators/mgmt/glusterd/src/glusterd-volgen.c
+++ b/xlators/mgmt/glusterd/src/glusterd-volgen.c
@@ -24,6 +24,7 @@
 #include "logging.h"
 #include "dict.h"
 #include "graph-utils.h"
+#include "common-utils.h"
 #include "glusterd-store.h"
 #include "glusterd-hooks.h"
 #include "trie.h"
@@ -3295,6 +3296,7 @@ volgen_graph_build_afr_clusters (volgen_graph_t *graph,
         if (volinfo->tier_info.cold_type == GF_CLUSTER_TYPE_REPLICATE)
                 start_count = volinfo->tier_info.cold_brick_count /
                               volinfo->tier_info.cold_replica_count;
+
         if (volinfo->tier_info.cur_tier_hot)
                 clusters = volgen_link_bricks_from_list_head_start (graph,
                                                 volinfo,
@@ -3572,6 +3574,7 @@ volume_volgen_graph_build_clusters_tier (volgen_graph_t *graph,
                 start_count = volinfo->tier_info.cold_brick_count /
                               volinfo->tier_info.cold_replica_count;
         }
+
         if (volinfo->dist_leaf_count != 1) {
                 ret = volgen_link_bricks_from_list_head_start
                         (graph, volinfo,
@@ -4227,11 +4230,11 @@ nfs_option_handler (volgen_graph_t *graph,
 }
 
 char*
-volgen_get_shd_key (glusterd_volinfo_t *volinfo)
+volgen_get_shd_key (int type)
 {
         char    *key = NULL;
 
-        switch (volinfo->type) {
+        switch (type) {
         case GF_CLUSTER_TYPE_REPLICATE:
         case GF_CLUSTER_TYPE_STRIPE_REPLICATE:
                 key = "cluster.self-heal-daemon";
@@ -4239,9 +4242,6 @@ volgen_get_shd_key (glusterd_volinfo_t *volinfo)
         case GF_CLUSTER_TYPE_DISPERSE:
                 key = "cluster.disperse-self-heal-daemon";
                 break;
-        case GF_CLUSTER_TYPE_TIER:
-                key = "cluster.tier-self-heal-daemon";
-                break;
         default:
                 key = NULL;
                 break;
@@ -4279,22 +4279,51 @@ volgen_graph_set_iam_shd (volgen_graph_t *graph)
 }
 
 static int
+prepare_shd_volume_options_for_tier (glusterd_volinfo_t *volinfo,
+                                     dict_t *set_dict)
+{
+        int             ret             = -1;
+        char           *key             = NULL;
+
+        key = volgen_get_shd_key (volinfo->tier_info.cold_type);
+        if (key) {
+                ret = dict_set_str (set_dict, key, "enable");
+                if (ret)
+                        goto out;
+        }
+
+        key = volgen_get_shd_key (volinfo->tier_info.hot_type);
+        if (key) {
+                ret = dict_set_str (set_dict, key, "enable");
+                if (ret)
+                        goto out;
+        }
+out:
+        return ret;
+}
+
+static int
 prepare_shd_volume_options (glusterd_volinfo_t *volinfo,
                             dict_t *mod_dict, dict_t *set_dict)
 {
         char    *key = NULL;
         int     ret = 0;
 
-        key = volgen_get_shd_key (volinfo);
-        if (!key) {
-                ret = -1;
-                goto out;
+        if (volinfo->type == GF_CLUSTER_TYPE_TIER) {
+                ret = prepare_shd_volume_options_for_tier (volinfo, set_dict);
+                if (ret)
+                        goto out;
+        } else {
+                key = volgen_get_shd_key (volinfo->type);
+                if (!key) {
+                        ret = -1;
+                        goto out;
+                }
+                ret = dict_set_str (set_dict, key, "enable");
+                if (ret)
+                        goto out;
         }
 
-        ret = dict_set_str (set_dict, key, "enable");
-        if (ret)
-                goto out;
-
         ret = dict_set_uint32 (set_dict, "trusted-client", GF_CLIENT_TRUSTED);
         if (ret)
                 goto out;
@@ -4405,6 +4434,42 @@ out:
         return clusters;
 }
 
+gf_boolean_t
+is_self_heal_enabled (glusterd_volinfo_t *volinfo, dict_t *dict)
+{
+
+        char            *shd_key                = NULL;
+        gf_boolean_t    shd_enabled             = _gf_false;
+
+        GF_VALIDATE_OR_GOTO ("glusterd", volinfo, out);
+
+        switch (volinfo->type) {
+        case GF_CLUSTER_TYPE_REPLICATE:
+        case GF_CLUSTER_TYPE_STRIPE_REPLICATE:
+        case GF_CLUSTER_TYPE_DISPERSE:
+                shd_key = volgen_get_shd_key (volinfo->type);
+                shd_enabled = dict_get_str_boolean (dict, shd_key,
+                                                    _gf_true);
+                break;
+        case GF_CLUSTER_TYPE_TIER:
+                shd_key = volgen_get_shd_key (volinfo->tier_info.cold_type);
+                if (shd_key)
+                        shd_enabled = dict_get_str_boolean (dict, shd_key,
+                                                            _gf_true);
+
+                shd_key = volgen_get_shd_key (volinfo->tier_info.hot_type);
+                if (shd_key)
+                        shd_enabled |= dict_get_str_boolean (dict, shd_key,
+                                                            _gf_true);
+
+                break;
+        default:
+                break;
+        }
+out:
+        return shd_enabled;
+}
+
 static int
 build_rebalance_volfile (glusterd_volinfo_t *volinfo, char *filepath,
                          dict_t *mod_dict)
diff --git a/xlators/mgmt/glusterd/src/glusterd-volgen.h b/xlators/mgmt/glusterd/src/glusterd-volgen.h
index ec96817..cf8a795 100644
--- a/xlators/mgmt/glusterd/src/glusterd-volgen.h
+++ b/xlators/mgmt/glusterd/src/glusterd-volgen.h
@@ -285,11 +285,12 @@ gd_is_boolean_option (char *key);
 
 
 char*
-volgen_get_shd_key (glusterd_volinfo_t *volinfo);
+volgen_get_shd_key (int type);
 
 int
 glusterd_volopt_validate (glusterd_volinfo_t *volinfo, dict_t *dict, char *key,
                           char *value, char **op_errstr);
-
+gf_boolean_t
+is_self_heal_enabled (glusterd_volinfo_t *volinfo, dict_t *dict);
 
 #endif
diff --git a/xlators/mgmt/glusterd/src/glusterd-volume-ops.c b/xlators/mgmt/glusterd/src/glusterd-volume-ops.c
index 83bcd0d..cfe2b01 100644
--- a/xlators/mgmt/glusterd/src/glusterd-volume-ops.c
+++ b/xlators/mgmt/glusterd/src/glusterd-volume-ops.c
@@ -674,14 +674,73 @@ out:
 
         return ret;
 }
-
 int
 glusterd_handle_cli_delete_volume (rpcsvc_request_t *req)
 {
         return glusterd_big_locked_handler (req,
                                             __glusterd_handle_cli_delete_volume);
 }
+static char*
+_get_shd_key (int type)
+{
+        char            *key               = NULL;
+
+        switch (type) {
+        case GF_CLUSTER_TYPE_REPLICATE:
+        case GF_CLUSTER_TYPE_STRIPE_REPLICATE:
+                key = "cluster.self-heal-daemon";
+                break;
+        case GF_CLUSTER_TYPE_DISPERSE:
+                key = "cluster.disperse-self-heal-daemon";
+                break;
+        default:
+                key = NULL;
+                break;
+        }
+        return key;
+}
+static int
+glusterd_handle_shd_option_for_tier (glusterd_volinfo_t *volinfo,
+                                     char *value, dict_t *dict)
+{
+        int             count           = 0;
+        char            dict_key[1024]  = {0, };
+        char           *key             = NULL;
+        int             ret             = 0;
+
+        key = _get_shd_key (volinfo->tier_info.cold_type);
+        if (key) {
+                count++;
+                snprintf (dict_key, sizeof (dict_key), "key%d", count);
+                ret = dict_set_str (dict, dict_key, key);
+                if (ret)
+                        goto out;
+                snprintf (dict_key, sizeof (dict_key), "value%d", count);
+                ret = dict_set_str (dict, dict_key, value);
+                if (ret)
+                        goto out;
+        }
+
+        key = _get_shd_key (volinfo->tier_info.hot_type);
+        if (key) {
+                count++;
+                snprintf (dict_key, sizeof (dict_key), "key%d", count);
+                ret = dict_set_str (dict, dict_key, key);
+                if (ret)
+                        goto out;
+                snprintf (dict_key, sizeof (dict_key), "value%d", count);
+                ret = dict_set_str (dict, dict_key, value);
+                if (ret)
+                        goto out;
+        }
+
+        ret = dict_set_int32 (dict, "count", count);
+        if (ret)
+                goto out;
 
+out:
+        return ret;
+}
 static int
 glusterd_handle_heal_enable_disable (rpcsvc_request_t *req, dict_t *dict,
                                      glusterd_volinfo_t *volinfo)
@@ -704,23 +763,31 @@ glusterd_handle_heal_enable_disable (rpcsvc_request_t *req, dict_t *dict,
                 goto out;
         }
 
-        key = volgen_get_shd_key (volinfo);
+        if (heal_op == GF_SHD_OP_HEAL_ENABLE) {
+                value = "enable";
+        } else if (heal_op == GF_SHD_OP_HEAL_DISABLE) {
+                value = "disable";
+        }
+
+       /* Convert this command to volume-set command based on volume type */
+        if (volinfo->type == GF_CLUSTER_TYPE_TIER) {
+                ret = glusterd_handle_shd_option_for_tier (volinfo, value,
+                                                           dict);
+                if (!ret)
+                        goto set_volume;
+                goto out;
+        }
+
+        key = volgen_get_shd_key (volinfo->type);
         if (!key) {
                 ret = -1;
                 goto out;
         }
 
-        /* Convert this command to volume-set command based on volume type */
         ret = dict_set_str (dict, "key1", key);
         if (ret)
                 goto out;
 
-        if (heal_op == GF_SHD_OP_HEAL_ENABLE) {
-                value = "enable";
-        } else if (heal_op == GF_SHD_OP_HEAL_DISABLE) {
-                value = "disable";
-        }
-
         ret = dict_set_str (dict, "value1", value);
         if (ret)
                 goto out;
@@ -729,6 +796,7 @@ glusterd_handle_heal_enable_disable (rpcsvc_request_t *req, dict_t *dict,
         if (ret)
                 goto out;
 
+set_volume:
         ret = glusterd_op_begin_synctask (req, GD_OP_SET_VOLUME, dict);
 
 out:
@@ -1826,9 +1894,7 @@ glusterd_op_stage_heal_volume (dict_t *dict, char **op_errstr)
                 ret = 0;
                 goto out;
         }
-
-        enabled = dict_get_str_boolean (opt_dict, volgen_get_shd_key (volinfo),
-                                        1);
+        enabled = is_self_heal_enabled (volinfo, opt_dict);
         if (!enabled) {
                 ret = -1;
                 snprintf (msg, sizeof (msg), "Self-heal-daemon is "
diff --git a/xlators/mgmt/glusterd/src/glusterd-volume-set.c b/xlators/mgmt/glusterd/src/glusterd-volume-set.c
index b90d3f1..8d37b8f 100644
--- a/xlators/mgmt/glusterd/src/glusterd-volume-set.c
+++ b/xlators/mgmt/glusterd/src/glusterd-volume-set.c
@@ -334,6 +334,16 @@ validate_disperse_heal_enable_disable (glusterd_volinfo_t *volinfo,
                                        char **op_errstr)
 {
         int                  ret = 0;
+        if (volinfo->type == GF_CLUSTER_TYPE_TIER) {
+               if (volinfo->tier_info.cold_type != GF_CLUSTER_TYPE_DISPERSE &&
+                   volinfo->tier_info.hot_type != GF_CLUSTER_TYPE_DISPERSE) {
+                        gf_asprintf (op_errstr, "Volume %s is not containing "
+                                     "disperse type", volinfo->volname);
+
+                       return -1;
+               } else
+                       return 0;
+        }
 
         if (volinfo->type != GF_CLUSTER_TYPE_DISPERSE) {
                 gf_asprintf (op_errstr, "Volume %s is not of disperse type",
-- 
1.7.1

